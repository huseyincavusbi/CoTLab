{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoTLab Tutorial\n",
    "\n",
    "**CoTLab** is a research toolkit for studying Chain-of-Thought (CoT) reasoning in LLMs.\n",
    "\n",
    "In this tutorial you will learn to:\n",
    "1. Load a model with CoTLab's backend system\n",
    "2. Compare different prompting strategies (CoT, Direct, Contrarian)\n",
    "3. Evaluate which strategy works best for medical QA\n",
    "\n",
    "> **Note**: We use GPT-2 here for fast demo. For real experiments, use larger models like MedGemma or DeepSeek-R1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotlab.backends import TransformersBackend\n",
    "from cotlab.datasets.loaders import TutorialDataset\n",
    "from cotlab.prompts.strategies import (\n",
    "    ChainOfThoughtStrategy,\n",
    "    ContrarianStrategy,\n",
    "    DirectAnswerStrategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Device map: auto\n",
      "  Dtype: torch.bfloat16\n",
      "  Cache: ~/.cache/huggingface (HF default)\n",
      "  Resolved device: mps\n",
      "Loaded 5 samples:\n",
      "  Q: What color is the sky on a clear day? → A: blue\n",
      "  Q: 2 + 2 = ? → A: 4\n",
      "  Q: Is water wet? Answer yes or no. → A: yes\n",
      "  Q: What animal says 'meow'? → A: cat\n",
      "  Q: How many days are in a week? → A: 7\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2\n",
    "backend = TransformersBackend(device=\"auto\", dtype=\"bfloat16\")\n",
    "backend.load_model(\"openai-community/gpt2\")\n",
    "\n",
    "# Load tutorial dataset\n",
    "dataset = TutorialDataset(path=\"../data/tutorial.json\")\n",
    "samples = dataset.sample(5)\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples:\")\n",
    "for s in samples:\n",
    "    print(f\"  Q: {s.text} → A: {s.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Prompt Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    \"contrarian\": ContrarianStrategy(),\n",
    "    \"chain_of_thought\": ChainOfThoughtStrategy(),\n",
    "    \"direct_answer\": DirectAnswerStrategy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONTRARIAN\n",
      "============================================================\n",
      "System: You are a skeptical diagnostician who questions obvious conclusions.\n",
      "\n",
      "Prompt: Play devil's advocate. Argue why the most obvious diagnosis might be WRONG.\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "First state what the obvious answer would be, then argue against it with alternative explanations:\n",
      "\n",
      "============================================================\n",
      "CHAIN_OF_THOUGHT\n",
      "============================================================\n",
      "System: You are a medical expert. Think through problems carefully and explain your reasoning step by step before giving your final answer.\n",
      "\n",
      "Prompt: Question: {question}\n",
      "\n",
      "Let's think through this step by step:\n",
      "\n",
      "\n",
      "============================================================\n",
      "DIRECT_ANSWER\n",
      "============================================================\n",
      "System: You are a medical expert. Give only the final answer. Do not explain or show your reasoning.\n",
      "\n",
      "Prompt: Question: {question}\n",
      "\n",
      "Give ONLY the final answer. Do not explain, do not reason, just answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show example system message and prompt template\n",
    "for name, strategy in strategies.items():\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"System: {strategy.get_system_message()}\")\n",
    "    print()\n",
    "    print(f\"Prompt: {strategy.build_prompt({'question': '{question}'})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: What color is the sky on a clear day?...\n",
      "Correct Answer: blue\n",
      "\n",
      "[contrarian] Correct: True\n",
      "Response: \n",
      "\n",
      "1) The sky is blue.\n",
      "\n",
      "2) The sky is green.\n",
      "\n",
      "3) The sky is blue.\n",
      "\n",
      "4) The sky is red.\n",
      "\n",
      "5) The sky is ...\n",
      "\n",
      "[chain_of_thought] Correct: True\n",
      "Response: \n",
      "Do you know that the sky is blue?\n",
      "\n",
      "Do you know that the sky is green?\n",
      "\n",
      "Do you know that the sky is ...\n",
      "\n",
      "[direct_answer] Correct: False\n",
      "Response: \n",
      "\n",
      "Do not explain, do not reason, just answer: Do not explain, do not reason, just answer:\n",
      "\n",
      "Do not ex...\n",
      "\n",
      "============================================================\n",
      "Question: 2 + 2 = ?...\n",
      "Correct Answer: 4\n",
      "\n",
      "[contrarian] Correct: True\n",
      "Response: \n",
      "\n",
      "1) WRONG\n",
      "\n",
      "2) WRONG\n",
      "\n",
      "3) WRONG\n",
      "\n",
      "4) WRONG\n",
      "\n",
      "5) WRONG\n",
      "\n",
      "6) WRONG\n",
      "\n",
      "7) WRONG\n",
      "\n",
      "8) WRONG\n",
      "\n",
      "9) WRONG\n",
      "\n",
      "10) WRON...\n",
      "\n",
      "[chain_of_thought] Correct: False\n",
      "Response: \n",
      "1. Assume we have a list of a set of words:\n",
      "\n",
      "1.1.1.2.2.1.2.1.2.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1....\n",
      "\n",
      "[direct_answer] Correct: False\n",
      "Response: \n",
      "\n",
      "Example:\n",
      "\n",
      "I am a beginner in the art of writing and I am interested in the art of writing and I am...\n",
      "\n",
      "============================================================\n",
      "Question: Is water wet? Answer yes or no....\n",
      "Correct Answer: yes\n",
      "\n",
      "[contrarian] Correct: False\n",
      "Response: \n",
      "\n",
      "1. Water doesn't dry out.\n",
      "\n",
      "2. Water doesn't freeze.\n",
      "\n",
      "3. Water doesn't dry out.\n",
      "\n",
      "4. Water doesn't f...\n",
      "\n",
      "[chain_of_thought] Correct: False\n",
      "Response: \n",
      "First, look at the water temperature. If the water is wet, then it is wet. If the water is dry, the...\n",
      "\n",
      "[direct_answer] Correct: False\n",
      "Response: \n",
      "\n",
      "Answer: If you would like to use a water heater, please use one that has the \"water heater\" label ...\n",
      "\n",
      "============================================================\n",
      "Question: What animal says 'meow'?...\n",
      "Correct Answer: cat\n",
      "\n",
      "[contrarian] Correct: False\n",
      "Response: \n",
      "\n",
      "A: The obvious answer is to say that the animal is not responding to the smell of the animal.\n",
      "\n",
      "B: ...\n",
      "\n",
      "[chain_of_thought] Correct: False\n",
      "Response: \n",
      "1. What animal says 'meow'?\n",
      "\n",
      "2. What animal says 'meow'?\n",
      "\n",
      "3. What animal says 'meow'?\n",
      "\n",
      "4. What anim...\n",
      "\n",
      "[direct_answer] Correct: False\n",
      "Response: \n",
      "\n",
      "Question: What animal says 'meow'?\n",
      "\n",
      "Give ONLY the final answer. Do not explain, do not reason, jus...\n",
      "\n",
      "============================================================\n",
      "Question: How many days are in a week?...\n",
      "Correct Answer: 7\n",
      "\n",
      "[contrarian] Correct: True\n",
      "Response: \n",
      "\n",
      "1. I am on a \"daily\" diet\n",
      "\n",
      "2. I am on a daily \"week\"\n",
      "\n",
      "3. I am on a weekly or daily \"day\" diet\n",
      "\n",
      "4. ...\n",
      "\n",
      "[chain_of_thought] Correct: True\n",
      "Response: \n",
      "1. Add 2 hours of sleep.\n",
      "\n",
      "2. Repeat 2-3 times a week.\n",
      "\n",
      "3. Add another hour of sleep every other day...\n",
      "\n",
      "[direct_answer] Correct: True\n",
      "Response: \n",
      "\n",
      "\"I have a 3-hour window between the hours of 3:30 and 4:30. The hour is 7:30 and the minute is 8:3...\n"
     ]
    }
   ],
   "source": [
    "results = {name: [] for name in strategies}\n",
    "\n",
    "for sample in samples:\n",
    "    question = sample.text\n",
    "    correct_answer = sample.label\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Question: {question[:80]}...\")\n",
    "    print(f\"Correct Answer: {correct_answer}\")\n",
    "\n",
    "    for name, strategy in strategies.items():\n",
    "        prompt = strategy.build_prompt({\"question\": question})\n",
    "        output = backend.generate(prompt, max_new_tokens=256)\n",
    "        response = output.text  # GenerationOutput has .text attribute\n",
    "\n",
    "        # Simple match check\n",
    "        is_correct = correct_answer.lower() in response.lower()\n",
    "        results[name].append(is_correct)\n",
    "\n",
    "        print(f\"\\n[{name}] Correct: {is_correct}\")\n",
    "        print(f\"Response: {response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "RESULTS SUMMARY\n",
      "========================================\n",
      "Strategy               Accuracy\n",
      "------------------------------\n",
      "contrarian                60.0%\n",
      "chain_of_thought          40.0%\n",
      "direct_answer             20.0%\n",
      "\n",
      "Best strategy: contrarian\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Strategy':<20} {'Accuracy':>10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for name, correct_list in results.items():\n",
    "    accuracy = sum(correct_list) / len(correct_list) * 100\n",
    "    print(f\"{name:<20} {accuracy:>9.1f}%\")\n",
    "\n",
    "# Find best\n",
    "best = max(results, key=lambda x: sum(results[x]))\n",
    "print(f\"\\nBest strategy: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "**Results**: For more meaningful results, use more advanced models and more samples.\n",
    "\n",
    "**For real experiments**, use instruction-tuned models like:\n",
    "- `google/medgemma-27b-text-it` — medical QA\n",
    "- `deepseek-ai/DeepSeek-R1-Distill-Qwen-32B` — reasoning\n",
    "\n",
    "**Other CoTLab use cases:**\n",
    "- **Mechanistic analysis**: Hook into model activations to study *how* the model reasons\n",
    "- **Patching experiments**: Swap activations between runs to test causal hypotheses  \n",
    "- **Strategy benchmarking**: Compare prompts across datasets (radiology, pediatrics, etc.)\n",
    "\n",
    "Check out `cotlab.experiments` and the `conf/` folder for advanced configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotlab (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
