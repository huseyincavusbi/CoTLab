defaults:
  - backend: vllm
  - model: medgemma_4b
  - prompt: chain_of_thought
  - dataset: synthetic
  - experiment: cot_faithfulness
  - _self_

# Global settings
seed: 42
verbose: true
dry_run: false

# Global prompt format override (plain, json, toon, toml, xml, yaml, markdown)
prompt:
  output_format: json

# Output configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}_${experiment.name}_${model.safe_name}_${oc.select:experiment.variants.0.name,${prompt.name}}_${prompt.output_format}_${oc.select:experiment.variants.1.name,${dataset.name}}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  output_subdir: null
