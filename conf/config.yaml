defaults:
  - backend: vllm
  - model: medgemma_4b
  - prompt: chain_of_thought
  - dataset: synthetic
  - experiment: cot_faithfulness
  - _self_

# Global settings
seed: 42
verbose: true
dry_run: false

# Global prompt format override (plain, json, toon, toml, xml, yaml, markdown)
prompt:
  output_format: plain

# Output configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}_${experiment.name}_${model.safe_name}_${prompt.name}_${prompt.output_format}_${dataset.name}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  output_subdir: null
