{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CoTLab","text":"<p>Chain of Thought research toolkit for LLM experiments.</p>"},{"location":"#what-it-does","title":"What It Does","text":"<ul> <li>Run mechanistic interpretability experiments on language models</li> <li>Test different prompt strategies on medical datasets</li> <li>Analyze attention heads, activations, and reasoning patterns</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>git clone https://github.com/huseyincavusbi/CoTLab.git\ncd CoTLab\nuv venv cotlab --python 3.11\nsource cotlab/bin/activate\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>python -m cotlab.main experiment=logit_lens model=medgemma_4b\npython -m cotlab.main experiment=cot_ablation dataset=pediatrics\npython -m cotlab.main prompt=radiology dataset=radiology\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>conf/           # Hydra configuration files\n  experiment/   # 14 experiment configs\n  model/        # 21 model configs\n  prompt/       # 19 prompt configs\n  dataset/      # 8 dataset configs\nsrc/cotlab/\n  experiments/  # Experiment implementations\n  prompts/      # Prompt strategies\n  backends/     # vLLM and Transformers backends\n  core/         # Base classes\ndata/           # Datasets (100 samples each)\n</code></pre>"},{"location":"#experiments","title":"Experiments","text":"Experiment Purpose <code>logit_lens</code> Layer-by-layer predictions <code>cot_ablation</code> Remove CoT tokens, measure effect <code>cot_heads</code> Find heads encoding reasoning <code>sycophancy_heads</code> Find sycophancy-related heads <code>activation_patching</code> Causal interventions <code>steering_vectors</code> Control behavior at inference"},{"location":"#models","title":"Models","text":"<p>Supports Gemma 3, MedGemma, DeepSeek-R1, Olmo-Think, and more.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"rocm-setup/","title":"AMD ROCm Setup Guide","text":"<p>Note: Tested on RDNA 4 in Ubuntu 24.04. Other setups may require different configuration.</p>"},{"location":"rocm-setup/#quick-start","title":"Quick Start","text":"<pre><code>./scripts/cotlab-rocm.sh model=gemma_270m\n</code></pre>"},{"location":"rocm-setup/#prerequisites","title":"Prerequisites","text":""},{"location":"rocm-setup/#1-rocm-drivers","title":"1. ROCm Drivers","text":"<p>Verify ROCm is installed and sees your GPU:</p> <pre><code>/opt/rocm/bin/rocminfo | grep gfx\n</code></pre>"},{"location":"rocm-setup/#2-docker-setup","title":"2. Docker Setup","text":"<pre><code># Add yourself to required groups\nsudo usermod -aG docker,video,render $USER\n\n# Activate\nnewgrp docker\n</code></pre>"},{"location":"rocm-setup/#3-huggingface-cache","title":"3. HuggingFace Cache","text":"<pre><code>mkdir -p ~/.cache/huggingface\n</code></pre>"},{"location":"rocm-setup/#how-it-works","title":"How It Works","text":"<p>CoTLab uses Docker with the official AMD vLLM image:</p> <pre><code>Host System (ROCm drivers)\n    \u2502\n    \u2514\u2500\u2500 Docker Container\n        \u251c\u2500\u2500 rocm/vllm-dev:rocm7.1.1_navi_...\n        \u251c\u2500\u2500 vLLM pre-compiled\n        \u2514\u2500\u2500 GPU access via /dev/kfd, /dev/dri\n</code></pre>"},{"location":"rocm-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"rocm-setup/#permission-denied","title":"Permission Denied","text":"<pre><code>sudo usermod -aG docker,video,render $USER\nnewgrp docker\n</code></pre>"},{"location":"rocm-setup/#gpu-stuck-at-100","title":"GPU Stuck at 100%","text":"<pre><code>sudo rocm-smi --resetclocks\n</code></pre>"},{"location":"rocm-setup/#known-limitations","title":"Known Limitations","text":"<p>vLLM does not support activation extraction (required for <code>logit_lens</code> and similar mechanistic experiments).</p>"},{"location":"api/core/","title":"Core API","text":""},{"location":"api/core/#cotlab.core.base.BasePromptStrategy","title":"<code>cotlab.core.base.BasePromptStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for prompt construction strategies.</p>"},{"location":"api/core/#cotlab.core.base.BasePromptStrategy.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Strategy name for logging.</p>"},{"location":"api/core/#cotlab.core.base.BasePromptStrategy.build_prompt","title":"<code>build_prompt(input_data)</code>  <code>abstractmethod</code>","text":"<p>Build a prompt from input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Dict[str, Any]</code> <p>Dictionary with at least 'question' key</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string</p>"},{"location":"api/core/#cotlab.core.base.BasePromptStrategy.get_compatible_datasets","title":"<code>get_compatible_datasets()</code>","text":"<p>Return list of compatible dataset names, or None if compatible with all.</p> <p>Override this in specialized prompts to restrict usage.</p>"},{"location":"api/core/#cotlab.core.base.BasePromptStrategy.get_system_message","title":"<code>get_system_message()</code>","text":"<p>Return system message if applicable.</p>"},{"location":"api/core/#cotlab.core.base.BasePromptStrategy.parse_response","title":"<code>parse_response(response)</code>  <code>abstractmethod</code>","text":"<p>Parse model response to extract answer and reasoning.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Raw model output</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with 'answer', 'reasoning', and any other extracted fields</p>"},{"location":"api/core/#cotlab.core.base.StructuredOutputMixin","title":"<code>cotlab.core.base.StructuredOutputMixin</code>","text":"<p>Mixin to add multi-format structured output capability to any prompt strategy.</p> <p>Supports: PLAIN, JSON, TOON, TOML, XML, YAML, MARKDOWN</p> Usage <p>class MyStrategy(StructuredOutputMixin, BasePromptStrategy):     def init(self, output_format=\"plain\", ...):         self.output_format = output_format         self.cot_format = False  # Set True for CoT inside structure</p>"},{"location":"api/core/#cotlab.core.base.ExperimentResult","title":"<code>cotlab.core.base.ExperimentResult</code>  <code>dataclass</code>","text":"<p>JSON-serializable experiment result.</p>"},{"location":"api/core/#cotlab.core.base.ExperimentResult.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load from JSON file.</p>"},{"location":"api/core/#cotlab.core.base.ExperimentResult.save","title":"<code>save(path)</code>","text":"<p>Save to JSON file.</p>"},{"location":"api/core/#cotlab.core.base.ExperimentResult.to_json","title":"<code>to_json()</code>","text":"<p>Serialize to JSON string.</p>"},{"location":"api/core/#cotlab.core.base.OutputFormat","title":"<code>cotlab.core.base.OutputFormat</code>","text":"<p>Supported output formats for structured responses.</p>"},{"location":"api/experiments/","title":"Experiments API","text":""},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter","title":"<code>cotlab.experiment.ExperimentDocumenter</code>","text":"<p>Generates markdown documentation for experiments.</p>"},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter.__init__","title":"<code>__init__(config, output_dir)</code>","text":""},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter.generate_title","title":"<code>generate_title()</code>","text":"<p>Generate human-readable experiment title from config.</p>"},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter.infer_research_questions","title":"<code>infer_research_questions()</code>","text":"<p>Infer research questions from configuration.</p>"},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter.create_initial_doc","title":"<code>create_initial_doc()</code>","text":"<p>Create initial experiment documentation (before results).</p>"},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter.update_with_results","title":"<code>update_with_results(results=None, duration_seconds=None)</code>","text":"<p>Update documentation with results after completion.</p>"},{"location":"api/experiments/#cotlab.experiment.ExperimentDocumenter.save","title":"<code>save(content=None)</code>","text":"<p>Save experiment documentation to file.</p>"},{"location":"api/prompts/","title":"Prompts API","text":""},{"location":"api/prompts/#specialty-strategies","title":"Specialty Strategies","text":""},{"location":"api/prompts/#cotlab.prompts.RadiologyPromptStrategy","title":"<code>cotlab.prompts.RadiologyPromptStrategy</code>","text":"<p>               Bases: <code>StructuredOutputMixin</code>, <code>BasePromptStrategy</code></p> <p>Structured JSON output for radiology pathological fracture detection.</p> <p>Uses structured JSON output format with: - Clear step-by-step reasoning instructions - JSON output with fracture_mentioned, pathological_fracture, evidence - Few-shot examples for format guidance</p>"},{"location":"api/prompts/#cotlab.prompts.RadiologyPromptStrategy.__init__","title":"<code>__init__(name='radiology', system_role=None, contrarian=False, few_shot=True, answer_first=False, output_format='json', **kwargs)</code>","text":""},{"location":"api/prompts/#cotlab.prompts.RadiologyPromptStrategy.build_prompt","title":"<code>build_prompt(input_data)</code>","text":"<p>Build prompt with radiology report.</p>"},{"location":"api/prompts/#cotlab.prompts.RadiologyPromptStrategy.parse_response","title":"<code>parse_response(response)</code>","text":"<p>Parse response from model (supports multiple formats).</p> <p>Expected format: {     \"fracture_mentioned\": bool,     \"pathological_fracture\": bool,     \"evidence\": {         \"report_findings\": [...],         \"rationale\": \"...\"     } }</p>"},{"location":"api/prompts/#cotlab.prompts.CardiologyPromptStrategy","title":"<code>cotlab.prompts.CardiologyPromptStrategy</code>","text":"<p>               Bases: <code>StructuredOutputMixin</code>, <code>BasePromptStrategy</code></p> <p>Structured JSON output for paediatric cardiology CHD detection.</p> <p>Uses structured JSON output format with: - Clear step-by-step reasoning instructions - JSON output with cardiac_abnormality, congenital_heart_defect, evidence - Few-shot examples for format guidance</p>"},{"location":"api/prompts/#cotlab.prompts.CardiologyPromptStrategy.build_prompt","title":"<code>build_prompt(input_data)</code>","text":"<p>Build prompt with cardiac imaging report.</p>"},{"location":"api/prompts/#cotlab.prompts.CardiologyPromptStrategy.get_compatible_datasets","title":"<code>get_compatible_datasets()</code>","text":"<p>Cardiology prompt is only compatible with cardiology dataset.</p> <p>This prompt is specifically designed for congenital heart defect detection in cardiac imaging reports and should NOT be used for general medical QA.</p>"},{"location":"api/prompts/#cotlab.prompts.CardiologyPromptStrategy.get_prediction_field","title":"<code>get_prediction_field()</code>","text":"<p>Return the JSON field name used for binary classification.</p>"},{"location":"api/prompts/#cotlab.prompts.CardiologyPromptStrategy.parse_response","title":"<code>parse_response(response)</code>","text":"<p>Parse response from model (supports multiple formats).</p> <p>Expected format: {     \"cardiac_abnormality\": bool,     \"congenital_heart_defect\": bool,     \"evidence\": {         \"report_findings\": [...],         \"rationale\": \"...\"     } }</p>"},{"location":"api/prompts/#cotlab.prompts.NeurologyPromptStrategy","title":"<code>cotlab.prompts.NeurologyPromptStrategy</code>","text":"<p>               Bases: <code>StructuredOutputMixin</code>, <code>BasePromptStrategy</code></p> <p>Structured JSON output for paediatric neurology abnormality detection.</p> <p>Uses structured JSON output format with: - Clear step-by-step reasoning instructions - JSON output with imaging_abnormality, neurological_abnormality, evidence - Few-shot examples for format guidance</p>"},{"location":"api/prompts/#cotlab.prompts.NeurologyPromptStrategy.build_prompt","title":"<code>build_prompt(input_data)</code>","text":"<p>Build prompt with neuroimaging report.</p>"},{"location":"api/prompts/#cotlab.prompts.NeurologyPromptStrategy.get_compatible_datasets","title":"<code>get_compatible_datasets()</code>","text":"<p>Neurology prompt is only compatible with neurology dataset.</p>"},{"location":"api/prompts/#cotlab.prompts.NeurologyPromptStrategy.get_prediction_field","title":"<code>get_prediction_field()</code>","text":"<p>Return the JSON field name used for binary classification.</p>"},{"location":"api/prompts/#cotlab.prompts.NeurologyPromptStrategy.parse_response","title":"<code>parse_response(response)</code>","text":"<p>Parse response from model (supports multiple formats).</p>"},{"location":"api/prompts/#cotlab.prompts.OncologyPromptStrategy","title":"<code>cotlab.prompts.OncologyPromptStrategy</code>","text":"<p>               Bases: <code>StructuredOutputMixin</code>, <code>BasePromptStrategy</code></p> <p>Structured JSON output for paediatric oncology malignancy detection.</p> <p>Uses structured JSON output format with: - Clear step-by-step reasoning instructions - JSON output with abnormal_findings, malignancy, evidence - Few-shot examples for format guidance</p>"},{"location":"api/prompts/#cotlab.prompts.OncologyPromptStrategy.get_compatible_datasets","title":"<code>get_compatible_datasets()</code>","text":"<p>Oncology prompt is only compatible with oncology dataset.</p>"},{"location":"api/prompts/#cotlab.prompts.OncologyPromptStrategy.get_prediction_field","title":"<code>get_prediction_field()</code>","text":"<p>Return the JSON field name used for binary classification.</p>"},{"location":"api/prompts/#cotlab.prompts.OncologyPromptStrategy.parse_response","title":"<code>parse_response(response)</code>","text":"<p>Parse response from model (supports multiple formats).</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>GPU recommended for model inference</li> </ul>"},{"location":"getting-started/installation/#install-with-uv-recommended","title":"Install with uv (Recommended)","text":"<pre><code>git clone https://github.com/huseyincavusbi/CoTLab.git\ncd CoTLab\nuv venv cotlab --python 3.11\nsource cotlab/bin/activate\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#install-with-pip","title":"Install with pip","text":"<pre><code>git clone https://github.com/huseyincavusbi/CoTLab.git\ncd CoTLab\npython -m venv cotlab\nsource cotlab/bin/activate\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#install-with-condamamba","title":"Install with conda/mamba","text":"<pre><code>git clone https://github.com/huseyincavusbi/CoTLab.git\ncd CoTLab\nconda create -n cotlab python=3.11\nconda activate cotlab\npip install -e \".[dev]\"\n</code></pre> <p>Or with mamba (faster):</p> <pre><code>mamba create -n cotlab python=3.11\nmamba activate cotlab\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup-vllm-backend","title":"GPU Setup (vLLM Backend)","text":"<p>CoTLab supports high-performance inference via vLLM. Installation varies by GPU.</p> <p>For AMD GPU users: See the comprehensive ROCm Setup Guide for detailed Docker-based installation.</p>"},{"location":"getting-started/installation/#nvidia-gpu-cuda","title":"NVIDIA GPU (CUDA)","text":"<pre><code># Standard installation - pulls CUDA-enabled vLLM from PyPI\nuv pip install vllm\n</code></pre>"},{"location":"getting-started/installation/#amd-gpu-rocm-docker-recommended","title":"AMD GPU (ROCm) - Docker (Recommended)","text":"<p>The official way to run vLLM on AMD GPUs is via Docker:</p> <pre><code># Run experiments using the ROCm Docker wrapper\n./scripts/cotlab-rocm.sh model=gemma_270m\n\n# First run downloads base image (~10 GB) and compiles kernels (~30 sec)\n# Subsequent runs start in seconds (cached)\n</code></pre> <p>Base Image: <code>rocm/vllm-dev:rocm7.1.1_navi_ubuntu24.04_py3.12_pytorch_2.8_vllm_0.10.2rc1</code> - Native RDNA 4 support - ROCm 7.1.1, PyTorch 2.8, vLLM 0.10.2</p> <p>Requirements: - Docker installed - ROCm drivers on host (<code>rocminfo</code> should show your GPU) - User in <code>docker</code>, <code>video</code>, and <code>render</code> groups</p>"},{"location":"getting-started/installation/#apple-silicon-metal","title":"Apple Silicon (Metal)","text":"<p>Requires Python 3.12 and the vllm-metal plugin:</p> <pre><code># Create venv with Python 3.12\nuv venv cotlab --python 3.12\nsource cotlab/bin/activate\nuv pip install -e \".[dev]\"\n\n# Install vLLM 0.13.0 from source (CPU build for macOS)\ncd /tmp\ncurl -OL https://github.com/vllm-project/vllm/releases/download/v0.13.0/vllm-0.13.0.tar.gz\ntar xf vllm-0.13.0.tar.gz &amp;&amp; cd vllm-0.13.0\nuv pip install -r requirements/cpu.txt --index-strategy unsafe-best-match\nuv pip install .\ncd -\n\n# Install vllm-metal plugin (MLX-accelerated)\nuv pip install vllm-metal\n\n# Verify\npython -c \"from vllm import LLM; import vllm_metal; print('Metal ready!')\"\n</code></pre> <p>Metal is auto-detected when you run with <code>backend=vllm</code> - no additional configuration needed.</p>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":"<p>Create <code>.env</code> file with your HuggingFace token:</p> <pre><code>HF_TOKEN=your_token_here\n</code></pre>"},{"location":"getting-started/installation/#verify","title":"Verify","text":"<pre><code>python -c \"import cotlab; print('OK')\"\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#running-experiments","title":"Running Experiments","text":"<pre><code># Logit lens on MedGemma\npython -m cotlab.main experiment=logit_lens model=medgemma_4b\n\n# CoT ablation on pediatrics dataset\npython -m cotlab.main experiment=cot_ablation dataset=pediatrics\n\n# Radiology classification\npython -m cotlab.main prompt=radiology dataset=radiology\n</code></pre>"},{"location":"getting-started/quickstart/#overriding-config","title":"Overriding Config","text":"<pre><code># Change model\npython -m cotlab.main experiment=logit_lens model=gemma_1b\n\n# Change backend\npython -m cotlab.main experiment=logit_lens backend=vllm\n\n# Multiple runs\npython -m cotlab.main -m prompt=radiology,cardiology\n</code></pre>"},{"location":"getting-started/quickstart/#batch-experiments","title":"Batch Experiments","text":"<p>Use Hydra multirun (<code>-m</code>) or a simple shell loop for batches.</p> <pre><code># Sweep prompt flags (Hydra multirun)\npython -m cotlab.main -m \\\n  experiment=classification \\\n  dataset=medqa \\\n  prompt=mcq \\\n  prompt.answer_first=true,false \\\n  prompt.few_shot=true \\\n  experiment.num_samples=20\n\n# Sweep datasets (Hydra multirun)\npython -m cotlab.main -m \\\n  experiment=classification \\\n  prompt=mcq \\\n  dataset=medqa,medmcqa,medxpertqa,mmlu_medical,afrimedqa \\\n  prompt.few_shot=true \\\n  experiment.num_samples=20\n\n# Shell loop (no Hydra multirun)\nfor ds in medqa medmcqa medxpertqa mmlu_medical afrimedqa; do\n  python -m cotlab.main experiment=classification dataset=$ds prompt=mcq prompt.few_shot=true experiment.num_samples=20\ndone\n</code></pre> <p>Multirun outputs are stored under <code>multirun/YYYY-MM-DD/HH-MM-SS/</code>.</p>"},{"location":"getting-started/quickstart/#output","title":"Output","text":"<p>Results saved to <code>outputs/YYYY-MM-DD/HH-MM-SS/</code>:</p> <ul> <li><code>results.json</code> - Predictions and metrics</li> <li><code>EXPERIMENT.md</code> - Run documentation</li> <li><code>config.yaml</code> - Config used</li> </ul>"},{"location":"guide/configuration/","title":"Configuration","text":"<p>Uses Hydra for configuration.</p>"},{"location":"guide/configuration/#config-files","title":"Config Files","text":"<pre><code>conf/\n\u251c\u2500\u2500 config.yaml        # Main config\n\u251c\u2500\u2500 experiment/        # 14 experiments\n\u251c\u2500\u2500 model/             # 21 models\n\u251c\u2500\u2500 prompt/            # 19 prompt strategies\n\u251c\u2500\u2500 dataset/           # 8 datasets\n\u2514\u2500\u2500 backend/           # vllm, transformers\n</code></pre>"},{"location":"guide/configuration/#cli-override","title":"CLI Override","text":"<pre><code>python -m cotlab.main \\\n    experiment=logit_lens \\\n    model=medgemma_4b \\\n    dataset=pediatrics \\\n    backend=transformers\n</code></pre>"},{"location":"guide/configuration/#prompt-parameters","title":"Prompt Parameters","text":"Parameter Type Description <code>few_shot</code> bool Include examples <code>answer_first</code> bool Conclude first, then justify <code>contrarian</code> bool Skeptical reasoning <code>output_format</code> str json/toml/yaml/xml/plain"},{"location":"guide/configuration/#using-custom-models","title":"Using Custom Models","text":"<p>CoTLab supports ANY vLLM-compatible model:</p> <pre><code># Use any model directly\npython -m cotlab.main model.name=meta-llama/Llama-3.1-8B\n\n# Override parameters\npython -m cotlab.main \\\n  model.name=Qwen/Qwen2.5-7B \\\n  model.max_tokens=4096\n</code></pre>"},{"location":"guide/configuration/#create-custom-config-optional","title":"Create Custom Config (Optional)","text":"<pre><code># Copy base template\ncp conf/model/_base/vllm_default.yaml conf/model/my_model.yaml\n\n# Edit parameters\n# Then use:\npython -m cotlab.main model=my_model\n</code></pre> <p>See Models Guide for compatibility details.</p>"},{"location":"guide/experiments/","title":"Experiments","text":""},{"location":"guide/experiments/#available-experiments","title":"Available Experiments","text":"Experiment Technique Purpose <code>logit_lens</code> Early decoding Layer-by-layer predictions <code>cot_ablation</code> Token ablation Zero CoT tokens, measure effect <code>cot_heads</code> Head patching Find heads encoding CoT <code>cot_faithfulness</code> Comparison Compare CoT vs direct answers <code>sycophancy_heads</code> Head patching Find sycophancy heads <code>activation_patching</code> Residual patching Causal interventions <code>steering_vectors</code> Activation steering Control behavior <code>full_layer_cot</code> Layer patching Patch full layers <code>probing_classifier</code> Probing Train probes on hidden states <code>radiology</code> Classification Medical report classification"},{"location":"guide/experiments/#running","title":"Running","text":"<pre><code>python -m cotlab.main experiment=logit_lens model=medgemma_4b\npython -m cotlab.main experiment=cot_ablation dataset=pediatrics\n</code></pre>"},{"location":"guide/experiments/#output","title":"Output","text":"<p>Each run creates:</p> <ul> <li><code>results.json</code> - Data and metrics</li> <li><code>EXPERIMENT.md</code> - Auto-generated documentation</li> <li><code>config.yaml</code> - Full config used</li> </ul>"},{"location":"guide/models/","title":"Models","text":""},{"location":"guide/models/#model-support","title":"Model Support","text":""},{"location":"guide/models/#cot-benchmarking-any-vllm-model","title":"CoT &amp; Benchmarking: ANY vLLM Model \u2713","text":"<p>CoTLab works with any model on HuggingFace that vLLM supports.</p> <pre><code># Use directly without config\npython -m cotlab.main model.name=meta-llama/Llama-3.1-8B experiment=cot_faithfulness\npython -m cotlab.main model.name=Qwen/Qwen2.5-7B experiment=radiology\npython -m cotlab.main model.name=mistralai/Mistral-7B-v0.1 experiment=cot_ablation\n</code></pre>"},{"location":"guide/models/#mechanistic-experiments-architecture-dependent","title":"Mechanistic Experiments: Architecture Dependent","text":"<p>Head patching, activation patching, and logit lens require standard Transformer architecture.</p>"},{"location":"guide/models/#pre-configured-models","title":"Pre-configured Models","text":"<p>CoTLab includes configs for commonly used models:</p> <pre><code>python -m cotlab.main model=medgemma_4b\npython -m cotlab.main model=gemma_1b\npython -m cotlab.main model=deepseek_r1_32b\n</code></pre> <p>See <code>conf/model/</code> for available configs.</p>"},{"location":"guide/models/#adding-new-models","title":"Adding New Models","text":""},{"location":"guide/models/#option-1-direct-override-quick","title":"Option 1: Direct Override (Quick)","text":"<p>No config file needed:</p> <pre><code>python -m cotlab.main model.name=your/model-name\n</code></pre>"},{"location":"guide/models/#option-2-create-config-recommended","title":"Option 2: Create Config (Recommended)","text":"<p>Use base templates:</p> <pre><code># Copy template\ncp conf/model/_base/vllm_default.yaml conf/model/my_model.yaml\n\n# Edit the file:\n# - Change 'name' to your model\n# - Adjust parameters as needed\n\n# Use it:\npython -m cotlab.main model=my_model\n</code></pre>"},{"location":"guide/models/#option-3-cli-helper-coming-soon","title":"Option 3: CLI Helper (Coming Soon)","text":"<pre><code>cotlab-template meta-llama/Llama-3.1-8B\n# Creates conf/model/meta_llama_llama_3_1_8b.yaml\n</code></pre>"},{"location":"guide/models/#vllm-compatibility","title":"vLLM Compatibility","text":"<p>Check vLLM supported models: https://docs.vllm.ai/en/latest/models/supported_models.html</p>"},{"location":"guide/prompts/","title":"Prompt Strategies","text":""},{"location":"guide/prompts/#general-prompts","title":"General Prompts","text":"<p>Located in <code>conf/prompt/</code>:</p> <ul> <li><code>chain_of_thought</code> - Step-by-step reasoning</li> <li><code>direct_answer</code> - Answer only</li> <li><code>sycophantic</code> - Suggest wrong answer</li> <li><code>adversarial</code> - Challenge the model</li> <li><code>uncertainty</code> - Confidence levels</li> <li><code>expert_persona</code> - Specialist personas</li> <li><code>few_shot</code> - Include examples</li> </ul>"},{"location":"guide/prompts/#medical-specialty-prompts","title":"Medical Specialty Prompts","text":"Prompt Task <code>radiology</code> Pathological fracture detection <code>cardiology</code> Congenital heart defect detection <code>neurology</code> Neurological abnormality detection <code>oncology</code> Malignancy detection"},{"location":"guide/prompts/#usage","title":"Usage","text":"<pre><code>python -m cotlab.main prompt=radiology dataset=radiology\npython -m cotlab.main prompt=cardiology dataset=cardiology\n</code></pre>"}]}